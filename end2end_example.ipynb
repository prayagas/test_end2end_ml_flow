{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19992470-2992-4ae2-9cc0-3005be61ddac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==\"2.19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e19ea77-e9eb-44b8-ab34-2ae8c7a731f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/saahiti.prayaga@gmail.com/test_end2end_ml_flow/00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00eb2a01-a721-4311-8f38-a79a8ef45ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pandas-on-Spark provides a Pandas-like API on top of PySpark, allowing you to use familiar Pandas functions while leveraging the distributed computing power of Spark. We'll use the pandas on spark API to scale pandas code. The Pandas instructions will be converted in the spark engine under the hood and distributed at scale.\n",
    "df = spark.table(\"mlops_churn_bronze_customers\").pandas_api()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05498c8a-ce13-4ae8-ac89-49bcbf0ac802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Engineering, data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5c90ee-0c8d-44a0-904d-c80f25aaf474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM mlops_churn_bronze_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebcf7813-4f9d-49d8-bbe0-eca840ff477e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"internet_service\"].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3e2ccd-6ef8-4df7-89eb-8161474f21f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def clean_churn_features(data_psdf: DataFrame) -> DataFrame:\n",
    "  \"\"\"\n",
    "  Simple cleaning function leveraging pandas API\n",
    "  \"\"\"\n",
    "\n",
    "  # Convert some columns\n",
    "  data_psdf = data_psdf.astype({\"senior_citizen\": \"string\"})\n",
    "  data_psdf[\"senior_citizen\"] = data_psdf[\"senior_citizen\"].map({\"1\" : \"Yes\", \"0\" : \"No\"})\n",
    "\n",
    "  data_psdf[\"total_charges\"] = data_psdf[\"total_charges\"].apply(lambda x: float(x) if x.strip() else 0)\n",
    "\n",
    "\n",
    "  # Fill some missing numerical values with 0\n",
    "  data_psdf = data_psdf.fillna({\"tenure\": 0.0})\n",
    "  data_psdf = data_psdf.fillna({\"monthly_charges\": 0.0})\n",
    "  data_psdf = data_psdf.fillna({\"total_charges\": 0.0})\n",
    "\n",
    "  def sum_optional_services(df):\n",
    "      \"\"\"Count number of optional services enabled, like streaming TV\"\"\"\n",
    "      cols = [\"online_security\", \"online_backup\", \"device_protection\", \"tech_support\",\n",
    "              \"streaming_tv\", \"streaming_movies\"]\n",
    "      return sum(map(lambda c: (df[c] == \"Yes\"), cols))\n",
    "\n",
    "  data_psdf[\"num_optional_services\"] = sum_optional_services(data_psdf)\n",
    "\n",
    "  # Return the cleaned Spark dataframe\n",
    "  return data_psdf.to_spark()\n",
    "\n",
    "churn_features = clean_churn_features(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2d8e199-dd88-42a3-a6df-889ca05b43b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we save the cleaned features and their labels as a DELTA LAKE TABLE "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7773744302745262,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "end2end_example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
