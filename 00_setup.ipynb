{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82c38d1-20e4-40c1-9e13-83af07a512d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"databricks-sdk>=0.28.0\" -qU\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d29d7d-a5e1-4447-bc8a-8c0ff31106e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "reformat_current_user = current_user.split(\"@\")[0].lower().replace(\".\", \"_\")\n",
    "\n",
    "catalog = \"main\"\n",
    "dbName = db = \"dbdemos_mlops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "986e178c-c06e-4dbc-bc3e-0c4094df8174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Helper Class DBDemos with useful functions to streamline common tasks in a Databricks ML workflow\n",
    "\n",
    "- The setup_schema method lets you create or reset a schema (database) and optionally create a volume. This is useful when you want to ensure your data environment is in a known state before running experiments.\n",
    "- The download_file_from_git method downloads files from a GitHub repository into a specified local folder. This can be used to pull in code, datasets, or configuration files that are maintained in Git.\n",
    "- The init_experiment_for_batch method creates a shared MLflow experiment (and the underlying folder) and sets the appropriate permissions using set_experiment_permission. This is helpful when you want a standardized way to start experiment tracking as part of your automated workflow.\n",
    "- The wait_for_table method polls for the existence and population of a specified table, ensuring that downstream processes only run when the data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "782b2278-fbb3-4a02-8b3e-99bc4325bb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class DBDemos():\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_schema(catalog, db, reset_all_data, volume_name=None):\n",
    "        \"\"\"\n",
    "        Sets up the Unity Catalog schema (database) and optionally drops and recreates it.\n",
    "        Optionally creates a volume if a volume name is provided.\n",
    "        \"\"\"\n",
    "        if reset_all_data:\n",
    "            print(f'Clearing volume: `{catalog}`.`{db}`.`{volume_name}`')\n",
    "            try:\n",
    "                spark.sql(f\"DROP VOLUME IF EXISTS `{catalog}`.`{db}`.`{volume_name}`\")\n",
    "                spark.sql(f\"DROP SCHEMA IF EXISTS `{catalog}`.`{db}` CASCADE\")\n",
    "            except Exception as e:\n",
    "                print(f'Catalog `{catalog}` or schema `{db}` do not exist. Skipping data reset.')\n",
    "\n",
    "        def use_and_create_db(catalog, dbName):\n",
    "            print(f\"Using catalog `{catalog}`\")\n",
    "            spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{dbName}`\")\n",
    "\n",
    "        assert catalog not in ['hive_metastore', 'spark_catalog'], \"This demo only supports Unity Catalog. Please change your catalog name.\"\n",
    "        current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0]['current_catalog()']\n",
    "        if current_catalog != catalog:\n",
    "            catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "            if catalog not in catalogs:\n",
    "                spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog}`\")\n",
    "                # Optionally set ownership if using a specific catalog name\n",
    "                if catalog == 'dbdemos':\n",
    "                    spark.sql(f\"ALTER CATALOG `{catalog}` OWNER TO `account users`\")\n",
    "        use_and_create_db(catalog, db)\n",
    "\n",
    "        print(f\"Using schema: `{catalog}`.`{db}`\")\n",
    "        spark.sql(f\"USE `{catalog}`.`{db}`\")\n",
    "\n",
    "        if volume_name:\n",
    "            spark.sql(f\"CREATE VOLUME IF NOT EXISTS {volume_name};\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_file_from_git(dest, owner, repo, path):\n",
    "        \"\"\"\n",
    "        Downloads files from a GitHub repository into the destination folder.\n",
    "        \"\"\"\n",
    "        def download_file(url, destination):\n",
    "            local_filename = url.split('/')[-1]\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                print(f\"Saving {destination}/{local_filename}\")\n",
    "                with open(os.path.join(destination, local_filename), 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "            return local_filename\n",
    "\n",
    "        if not os.path.exists(dest):\n",
    "            os.makedirs(dest)\n",
    "        \n",
    "        files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "        files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "        \n",
    "        def download_to_dest(url):\n",
    "            try:\n",
    "                # Optionally switch to an alternate URL if needed\n",
    "                s3url = url.replace(\"https://raw.githubusercontent.com/databricks-demos/dbdemos-dataset/main/\", \n",
    "                                      \"https://dbdemos-dataset.s3.amazonaws.com/\")\n",
    "                download_file(s3url, dest)\n",
    "            except Exception:\n",
    "                download_file(url, dest)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            list(executor.map(download_to_dest, files))\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_experiment_for_batch(demo_name, experiment_name):\n",
    "        \"\"\"\n",
    "        Initializes an MLflow experiment in a shared folder and sets permissions.\n",
    "        \"\"\"\n",
    "        import mlflow\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        \n",
    "        w = WorkspaceClient()\n",
    "        xp_root_path = f\"/Shared/dbdemos/experiments/{demo_name}\"\n",
    "        try:\n",
    "            w.workspace.mkdirs(path=xp_root_path)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Couldn't create folder for experiment under {xp_root_path}. Please create it manually or skip init. Error: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        xp = f\"{xp_root_path}/{experiment_name}\"\n",
    "        print(f\"Using experiment: {xp}\")\n",
    "        mlflow.set_experiment(xp)\n",
    "        DBDemos.set_experiment_permission(xp)\n",
    "        return mlflow.get_experiment_by_name(xp)\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_experiment_permission(experiment_path):\n",
    "        \"\"\"\n",
    "        Sets permissions on the experiment folder so that all users can manage it.\n",
    "        \"\"\"\n",
    "        from databricks.sdk import WorkspaceClient\n",
    "        from databricks.sdk.service import iam\n",
    "        \n",
    "        w = WorkspaceClient()\n",
    "        try:\n",
    "            status = w.workspace.get_status(experiment_path)\n",
    "            w.permissions.set(\"experiments\", request_object_id=status.object_id, access_control_list=[\n",
    "                iam.AccessControlRequest(group_name=\"users\", permission_level=iam.PermissionLevel.CAN_MANAGE)\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting permissions for experiment {experiment_path}: {e}\")\n",
    "        print(f\"Experiment {experiment_path} permissions set to public (users CAN_MANAGE).\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def wait_for_table(table_name, timeout_duration=120):\n",
    "        \"\"\"\n",
    "        Waits for a table to exist and be non-empty, or raises an exception after timeout.\n",
    "        \"\"\"\n",
    "        import time\n",
    "        i = 0\n",
    "        while not spark.catalog.tableExists(table_name) or spark.table(table_name).count() == 0:\n",
    "            time.sleep(1)\n",
    "            i += 1\n",
    "            if i > timeout_duration:\n",
    "                raise Exception(f\"Could not find table {table_name} or table is empty.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b35ad67-dba3-4e7b-988f-15b0a71862b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)\n",
    "\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# Set UC Model Registry as default\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# create an instance of MLflowClient (assigned to client), which allows subsequent code to interact with MLflow for tracking experiments and managing models.\n",
    "client = MlflowClient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "325219aa-70fb-4235-87f1-346d90c6fcb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Setting up a bronze layer table for the ML Flow\n",
    "Steps\n",
    "\n",
    "1) Download dataset (csv file) from GitHub\n",
    "2) Read into a pandas dataframe, clean data\n",
    "3) Create a bronze table: Convert cleaned pandas dataframe into a table in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2281bf14-45e9-479c-b4b0-29befc9d8820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_table_name = \"mlops_churn_bronze_customers\"\n",
    "\n",
    "if not spark.catalog.tableExists(bronze_table_name):\n",
    "    import requests \n",
    "    from io import StringIO\n",
    "    #Dataset under apache license: https://github.com/IBM/telco-customer-churn-on-icp4d/blob/master/LICENSE\n",
    "    csv = requests.get(\"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\").text\n",
    "    df = pd.read_csv(StringIO(csv), sep=\",\")\n",
    "\n",
    "    def cleanup_column(pdf):\n",
    "        # Clean up column names\n",
    "        pdf.columns = [re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower().replace(\"__\", \"_\") for name in pdf.columns]\n",
    "        pdf.columns = [re.sub(r'[\\(\\)]', '', name).lower() for name in pdf.columns]\n",
    "        pdf.columns = [re.sub(r'[ -]', '_', name).lower() for name in pdf.columns]\n",
    "        return pdf.rename(columns = {'streaming_t_v': 'streaming_tv', 'customer_i_d': 'customer_id'})\n",
    "    \n",
    "    df = cleanup_column(df)\n",
    "    print(f\"creating `{bronze_table_name}` raw table\")\n",
    "    #convert the in-memory (pandas) data into a persistant Spark table --> becomes a managed table in the databricks environment, making it accessible to other notebooks and jobs. It also enforces schema consistency and guarantees that any previous version of the table is replaced with new data\n",
    "    spark.createDataFrame(df).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(bronze_table_name)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d084ca8a-5ccc-49c4-9204-ee0b64dc8358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Integrating and automating AutoML into the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb975a5-334e-4ef2-8986-ada32e202f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#from databricks.feature_store import FeatureStoreClient\n",
    "import mlflow\n",
    "import databricks\n",
    "from datetime import datetime\n",
    "\n",
    "def get_automl_run(name):\n",
    "  #get the most recent automl run\n",
    "  df = spark.table(\"field_demos_metadata.automl_experiment\").filter(col(\"name\") == name).orderBy(col(\"date\").desc()).limit(1)\n",
    "  return df.collect()\n",
    "\n",
    "#Get the automl run information from the field_demos_metadata.automl_experiment table. \n",
    "#If it's not available in the metadata table, start a new run with the given parameters\n",
    "def get_automl_run_or_start(name, model_name, dataset, target_col, timeout_minutes):\n",
    "  spark.sql(\"create database if not exists field_demos_metadata\")\n",
    "  spark.sql(\"create table if not exists field_demos_metadata.automl_experiment (name string, date string)\")\n",
    "  result = get_automl_run(name)\n",
    "  if len(result) == 0:\n",
    "    print(\"No run available, start a new Auto ML run, this will take a few minutes...\")\n",
    "    start_automl_run(name, model_name, dataset, target_col, timeout_minutes)\n",
    "    result = get_automl_run(name)\n",
    "  return result[0]\n",
    "\n",
    "\n",
    "#Start a new auto ml classification task and save it as metadata.\n",
    "def start_automl_run(name, model_name, dataset, target_col, timeout_minutes = 5):\n",
    "  automl_run = databricks.automl.classify(\n",
    "    dataset = dataset,\n",
    "    target_col = target_col,\n",
    "    timeout_minutes = timeout_minutes\n",
    "  )\n",
    "  experiment_id = automl_run.experiment.experiment_id\n",
    "  path = automl_run.experiment.name\n",
    "  data_run_id = mlflow.search_runs(experiment_ids=[automl_run.experiment.experiment_id], filter_string = \"tags.mlflow.source.name='Notebook: DataExploration'\").iloc[0].run_id\n",
    "  exploration_notebook_id = automl_run.experiment.tags[\"_databricks_automl.exploration_notebook_id\"]\n",
    "  best_trial_notebook_id = automl_run.experiment.tags[\"_databricks_automl.best_trial_notebook_id\"]\n",
    "\n",
    "  cols = [\"name\", \"date\", \"experiment_id\", \"experiment_path\", \"data_run_id\", \"best_trial_run_id\", \"exploration_notebook_id\", \"best_trial_notebook_id\"]\n",
    "  spark.createDataFrame(data=[(name, datetime.today().isoformat(), experiment_id, path, data_run_id, automl_run.best_trial.mlflow_run_id, exploration_notebook_id, best_trial_notebook_id)], schema = cols).write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"field_demos_metadata.automl_experiment\")\n",
    "  #Create & save the first model version in the MLFlow repo (required to setup hooks etc)\n",
    "  mlflow.register_model(f\"runs:/{automl_run.best_trial.mlflow_run_id}/model\", model_name)\n",
    "  return get_automl_run(name)\n",
    "\n",
    "#Generate nice link for the given auto ml run\n",
    "def display_automl_link(name, model_name, dataset, target_col, force_refresh=False, timeout_minutes = 5):\n",
    "  r = get_automl_run_or_start(name, model_name, dataset, target_col, timeout_minutes)\n",
    "  html = f\"\"\"For exploratory data analysis, open the <a href=\"/#notebook/{r[\"exploration_notebook_id\"]}\">data exploration notebook</a><br/><br/>\"\"\"\n",
    "  html += f\"\"\"To view the best performing model, open the <a href=\"/#notebook/{r[\"best_trial_notebook_id\"]}\">best trial notebook</a><br/><br/>\"\"\"\n",
    "  html += f\"\"\"To view details about all trials, navigate to the <a href=\"/#mlflow/experiments/{r[\"experiment_id\"]}/s?orderByKey=metrics.%60val_f1_score%60&orderByAsc=false\">MLflow experiment</>\"\"\"\n",
    "  displayHTML(html)\n",
    "\n",
    "\n",
    "def display_automl_churn_link(): \n",
    "  display_automl_link(\"churn_auto_ml\", \"field_demos_customer_churn\", spark.table(\"churn_features\"), \"churn\", 5)\n",
    "\n",
    "def get_automl_churn_run(): \n",
    "  return get_automl_run_or_start(\"churn_auto_ml\", \"field_demos_customer_churn\", spark.table(\"churn_features\"), \"churn\", 5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
